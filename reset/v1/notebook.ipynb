{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following is heavily inspired by Anas' work as it served as a basis and an introduction to pyTorch (coming from Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDFA(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, hidden_layers_number=1):\n",
    "        \n",
    "        super(SimpleDFA, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers_number = hidden_layers_number\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Use ModuleList for layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(input_size, hidden_size)] + \n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers_number)] + \n",
    "            [nn.Linear(hidden_size, output_size)]\n",
    "        )\n",
    "        \n",
    "        self.activation = nn.Tanh()  # Use Tanh activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply activation to each hidden layer\n",
    "        for layer in self.layers[:-1]:  # All except the last layer\n",
    "            x = self.activation(layer(x))\n",
    "        # Final layer (output layer) without activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def Init_weights(self):\n",
    "        # Xavier init on weights\n",
    "        for layer in self.layers: \n",
    "            nn.init.xavier_uniform_(layer.weight) \n",
    "\n",
    "    \n",
    "    def Init_feedback(self):\n",
    "        # Create feedback matrices matching each layer's output size\n",
    "        self.B_hiddens = [torch.randn(self.hidden_size, self.hidden_size) * 0.01 for _ in range(self.hidden_layers_number)]\n",
    "        self.B_output = torch.randn(self.output_size, self.hidden_size) * 0.01  # Correct feedback matrix for output layer\n",
    "\n",
    "        # Normalize feedback matrices\n",
    "        for i in range(len(self.B_hiddens)):\n",
    "            self.B_hiddens[i] /= self.B_hiddens[i].norm()\n",
    "        self.B_output /= self.B_output.norm()\n",
    "\n",
    "        return self.B_hiddens + [self.B_output]  # Feedback matrices for hidden layers + output\n",
    "\n",
    "\n",
    "\n",
    "    def forward_pass(self,Input,Target,loss_func):\n",
    "        \n",
    "        # Forward pass\n",
    "        Output = self.forward(Input)\n",
    "        loss = loss_func(Output, Target)\n",
    "        return Output, loss\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        \n",
    "        return 1 - self.activation(x)**2  # Derivative of Tanh activation function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFA training step (UPDATING IN PROGRESS)\n",
    "\n",
    "check what's up with hidden_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa_update_step(model, Bs, output, target, hidden_activations, inputs, learning_rate=0.01):\n",
    "    # Compute error at the output\n",
    "    error = output - target\n",
    "\n",
    "    # Compute feedback signal for output layer\n",
    "    feedback_signal_output = torch.matmul(error * model.activation_derivative(output), Bs[-1])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Update output layer weights and biases\n",
    "        delta_w_out = torch.matmul(feedback_signal_output.T, hidden_activations[-1])\n",
    "        model.layers[-1].weight -= learning_rate * delta_w_out\n",
    "        delta_b_out = feedback_signal_output.sum(0)\n",
    "        model.layers[-1].bias -= learning_rate * delta_b_out\n",
    "\n",
    "    # Propagate through hidden layers in reverse order\n",
    "    for index in reversed(range(len(model.layers) - 1)):\n",
    "        # Compute feedback signal for current hidden layer\n",
    "        feedback_signal_hidden = torch.matmul(error, Bs[index]) * model.activation_derivative(hidden_activations[index])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Update weights and biases for hidden layers\n",
    "            delta_w = torch.matmul(feedback_signal_hidden.T, hidden_activations[index - 1] if index > 0 else inputs)\n",
    "            model.layers[index].weight -= learning_rate * delta_w\n",
    "            delta_b = feedback_signal_hidden.sum(0)\n",
    "            model.layers[index].bias -= learning_rate * delta_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, targets, loss_fn, learning_rate=0.01, epochs=1000):\n",
    "    # Initialize feedback matrices\n",
    "    Bs = model.Init_feedback()\n",
    "\n",
    "    losses = []  # To store the loss at each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "\n",
    "        # Pass data through each hidden layer and store activations\n",
    "        hidden_activations = []\n",
    "        x = data\n",
    "        for layer in model.layers[:-1]:  # Collect activations for hidden layers only\n",
    "            x = model.activation(layer(x))\n",
    "            hidden_activations.append(x)\n",
    "\n",
    "        output, loss = model.forward_pass(data, targets, loss_fn)\n",
    "\n",
    "        # Apply DFA update step\n",
    "        dfa_update_step(model, Bs, output, targets, hidden_activations, data, learning_rate)\n",
    "\n",
    "        # Store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print loss and accuracy every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = (output >= 0.5).float()  # Threshold the output to get binary predictions\n",
    "            correct = (predictions == targets).float().sum()\n",
    "            accuracy = correct / targets.size(0)\n",
    "            print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Plot the convergence of loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Convergence of Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the network using DFA and print accuracy\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, targets, loss_fn, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_pass(data, targets, loss_fn)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply DFA update step\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdfa_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Store the loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mdfa_update_step\u001b[0;34m(model, Bs, output, target, hidden_activations, inputs, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Update output layer weights and biases\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     delta_w_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(feedback_signal_output\u001b[38;5;241m.\u001b[39mT, hidden_activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_w_out\u001b[49m\n\u001b[1;32m     12\u001b[0m     delta_b_out \u001b[38;5;241m=\u001b[39m feedback_signal_output\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m delta_b_out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 6 # Hidden layers size\n",
    "hidden_layers_number = 1 # Number of hidden layers\n",
    "output_size = 2  \n",
    "\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)  # XOR inputs\n",
    "targets = torch.tensor([[0, 0], [1, 0], [1, 0], [0, 0]], dtype=torch.float32)  # XOR targets\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and loss function\n",
    "model = SimpleDFA(input_size, hidden_size, output_size, hidden_layers_number)\n",
    "model.Init_weights()  # Initialize the weights\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train the network using DFA and print accuracy\n",
    "train(model, data, targets, loss_fn, learning_rate=0.1, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
