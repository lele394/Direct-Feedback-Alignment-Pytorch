{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple single layer Neural network with DFA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work from Anas. Served as a basis for my own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture with a hidden layer and Tanh activation\n",
    "class SimpleDFA(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        super(SimpleDFA, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input weight matrix (input to hidden)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output weight matrix (hidden to output)\n",
    "        self.activation = nn.Tanh()  # Use Tanh activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.activation(self.fc1(x))  # Hidden layer with Tanh activation\n",
    "        return self.activation(self.fc2(x))  # Output layer with Tanh activation\n",
    "    \n",
    "    \n",
    "    def Init_weights(self):\n",
    "        # Xavier init on weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight) \n",
    "        nn.init.xavier_uniform_(self.fc2.weight) \n",
    "\n",
    "        print(self.fc1.weight)\n",
    "        print(self.fc2.weight)\n",
    "        \n",
    "    \n",
    "    def Init_feedback(self):\n",
    "        \n",
    "        self.B_1 = torch.randn(self.output_size, self.hidden_size) * 0.01  # Feedback for 1st layer\n",
    "        self.B_2 = torch.randn(self.hidden_size, self.input_size) * 0.01 # Feedback for 2nd layer\n",
    "        \n",
    "        #normalize the feedback matrices to avoid exploding feedback\n",
    "        self.B_1 = self.B_1 / self.B_1.norm()\n",
    "        self.B_2 = self.B_2 / self.B_2.norm()\n",
    "        return self.B_1, self.B_2\n",
    "    \n",
    "    def forward_pass(self,Input,Target,loss_func):\n",
    "        \n",
    "        # Forward pass\n",
    "        Output = self.forward(Input)\n",
    "        loss = loss_func(Output, Target)\n",
    "        return Output, loss\n",
    "        \n",
    "    def activation_derivative(self, x):\n",
    "        \n",
    "        return 1 - self.activation(x)**2  # Derivative of Tanh activation function\n",
    "    \n",
    "    \n",
    "def dfa_update_step(model, feedback_matrix_output, feedback_matrix_input, output, target, hidden_activations, inputs, learning_rate=0.01):\n",
    "    #function to update the weights and biases using DFA\n",
    "\n",
    "    error = output - target  # Compute error\n",
    "    \n",
    "    # update the output weights and biases at the output\n",
    "    feedback_signal_output = torch.matmul(error * model.activation_derivative(output), feedback_matrix_output)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #weight update for the output layer\n",
    "        delta_w_out = torch.matmul(feedback_signal_output.T, hidden_activations)\n",
    "        model.fc2.weight -= learning_rate * delta_w_out\n",
    "        #bias update for the output layer\n",
    "        delta_b_out = feedback_signal_output.sum(0)\n",
    "        model.fc2.bias -= learning_rate * delta_b_out\n",
    "        \n",
    "    print(\"output no problem\")\n",
    "\n",
    "    # update the input to hidden weights and biases\n",
    "    feedback_signal_input = torch.matmul(error * model.activation_derivative(hidden_activations), feedback_matrix_input)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #weight update for the input layer\n",
    "        delta_w_in = torch.matmul(feedback_signal_input.T, inputs)\n",
    "        model.fc1.weight -= learning_rate * delta_w_in\n",
    "        #bias update for the input layer\n",
    "        delta_b_in = feedback_signal_input.sum(0)\n",
    "        model.fc1.bias -= learning_rate * delta_b_in\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "def fa_update_step(model, feedback_matrix_output, feedback_matrix_input, output, target, hidden_activations, inputs, learning_rate=0.01):\n",
    "    #function to update the weights and biases using simple FA\n",
    "    error = output - target  # Compute error\n",
    "    \n",
    "    # update the output weights and biases at the output\n",
    "    feedback_signal_output = torch.matmul(error * model.activation_derivative(output), feedback_matrix_output)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #weight update for the output layer\n",
    "        delta_w_out = torch.matmul(feedback_signal_output.T, hidden_activations)\n",
    "        model.fc2.weight -= learning_rate * delta_w_out\n",
    "        #bias update for the output layer\n",
    "        delta_b_out = feedback_signal_output.sum(0)\n",
    "        model.fc2.bias -= learning_rate * delta_b_out\n",
    "        \n",
    "    # update the input to hidden weights and biases\n",
    "    feedback_signal_input = torch.matmul(error * model.activation_derivative(hidden_activations), feedback_matrix_input)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #weight update for the input layer\n",
    "        delta_w_in = torch.matmul(feedback_signal_input.T, inputs)\n",
    "        model.fc1.weight -= learning_rate * delta_w_in\n",
    "        #bias update for the input layer\n",
    "        delta_b_in = feedback_signal_input.sum(0)\n",
    "        model.fc1.bias -= learning_rate * delta_b_in\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# Credits : https://github.com/iacolippo/Direct-Feedback-Alignment/blob/master/dfa-mnist.ipynb\n",
    "\n",
    "def dfa_found_update_step(model, feedback_matrix_output, feedback_matrix_input, output, target, hidden_activations, inputs, learning_rate=0.01):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"feedback_matrix_output\", feedback_matrix_output.size())\n",
    "    print(\"feedback_matrix_input\", feedback_matrix_input.size())\n",
    "    print(\"output\", output.size())\n",
    "    print(\"inputs\", inputs.size())\n",
    "    print(\"target\", target.size())\n",
    "    print(\"hidden_activations\", hidden_activations.size())\n",
    "\n",
    "\n",
    "\n",
    "    # Compute error at the output layer\n",
    "    e = output - target  # Error signal\n",
    "    \n",
    "    # Generate feedback-aligned signal for the output layer using feedback_matrix_output\n",
    "    da2 = torch.matmul(feedback_matrix_output.T, e.T)  # (output_size, batch_size)\n",
    "    \n",
    "    # Compute weight gradient for the output layer using DFA\n",
    "    dW2 = -torch.matmul(da2.T, hidden_activations)  # (output_size, batch_size) * (batch_size, hidden_size) -> (output_size, hidden_size)\n",
    "    db2 = -torch.sum(da2, dim=1, keepdim=True)  # Sum along batch dimension -> (output_size, 1)\n",
    "    \n",
    "    # Generate feedback-aligned signal for the hidden layer using feedback_matrix_input\n",
    "    da1 = torch.matmul(feedback_matrix_input.T, e.T) * (1 - torch.tanh(hidden_activations) ** 2)  # (hidden_size, batch_size)\n",
    "    \n",
    "    # Compute weight gradients for the input-to-hidden layer using inputs\n",
    "    dW1 = -torch.matmul(da1.T, inputs)  # (batch_size, hidden_size) * (batch_size, input_size) -> (hidden_size, input_size)\n",
    "    db1 = -torch.sum(da1, dim=1, keepdim=True)  # Sum along batch dimension -> (hidden_size, 1)\n",
    "\n",
    "    # Update the model's weights and biases\n",
    "    with torch.no_grad():\n",
    "        model.fc2.weight -= learning_rate * dW2  # Update hidden-to-output weights\n",
    "        model.fc2.bias -= learning_rate * db2.squeeze()  # Update output layer bias\n",
    "        \n",
    "        model.fc1.weight -= learning_rate * dW1  # Update input-to-hidden weights\n",
    "        model.fc1.bias -= learning_rate * db1.squeeze()  # Update hidden layer bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, targets, loss_fn, learning_rate=0.01, epochs=1000):\n",
    "    # Initialize feedback matrices\n",
    "    B_1, B_2 = model.Init_feedback()\n",
    "\n",
    "    losses = []  # To store the loss at each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        hidden_activations = model.activation(model.fc1(data))  # Get hidden layer activations\n",
    "        output, loss = model.forward_pass(data, targets, loss_fn)\n",
    "\n",
    "        # Apply DFA update step\n",
    "        # dfa_update_step(model, B_1, B_2, output, targets, hidden_activations, data, learning_rate)\n",
    "        dfa_found_update_step(model, B_1, B_2, output, targets, hidden_activations, data, learning_rate)\n",
    "\n",
    "        # Store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Print loss and accuracy every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = (output >= 0.5).float()  # Threshold the output to get binary predictions\n",
    "            correct = (predictions == targets).float().sum()\n",
    "            accuracy = correct / targets.size(0)\n",
    "            print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Plot the convergence of loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Convergence of Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the code on a simple XOR task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0299, -0.4139, -0.3307],\n",
      "        [-0.5169,  0.2924,  0.8625],\n",
      "        [-0.6726,  0.5981, -0.0745],\n",
      "        [-0.4026, -0.3973, -0.4581]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.9295,  0.0952, -0.5856,  0.2938],\n",
      "        [-0.4500, -0.2215,  0.0379, -0.3219]], requires_grad=True)\n",
      "feedback_matrix_output torch.Size([2, 4])\n",
      "feedback_matrix_input torch.Size([4, 3])\n",
      "output torch.Size([4, 2])\n",
      "inputs torch.Size([4, 3])\n",
      "target torch.Size([4, 2])\n",
      "hidden_activations torch.Size([4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x4 and 2x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the network using DFA and print XOR accuracy\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[132], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, targets, loss_fn, learning_rate, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_pass(data, targets, loss_fn)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Apply DFA update step\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# dfa_update_step(model, B_1, B_2, output, targets, hidden_activations, data, learning_rate)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdfa_found_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Store the loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[131], line 30\u001b[0m, in \u001b[0;36mdfa_found_update_step\u001b[0;34m(model, feedback_matrix_output, feedback_matrix_input, output, target, hidden_activations, inputs, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(da2, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Sum along batch dimension -> (output_size, 1)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Generate feedback-aligned signal for the hidden layer using feedback_matrix_input\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m da1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeedback_matrix_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(hidden_activations) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (hidden_size, batch_size)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compute weight gradients for the input-to-hidden layer using inputs\u001b[39;00m\n\u001b[1;32m     33\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(da1\u001b[38;5;241m.\u001b[39mT, inputs)  \u001b[38;5;66;03m# (batch_size, hidden_size) * (batch_size, input_size) -> (hidden_size, input_size)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x4 and 2x4)"
     ]
    }
   ],
   "source": [
    "# Example usage with XOR data\n",
    "input_size = 3  # XOR has two input features (0 or 1)\n",
    "hidden_size = 4  # You can adjust this value for the hidden layer\n",
    "output_size = 2  # XOR has one output feature (0 or 1)\n",
    "\n",
    "\n",
    "data = torch.tensor([[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 0]], dtype=torch.float32)  # XOR inputs\n",
    "targets = torch.tensor([[0, 0], [1, 0], [1, 0], [0, 0]], dtype=torch.float32)  # XOR targets\n",
    "\n",
    "# Initialize the model and loss function\n",
    "model = SimpleDFA(input_size, hidden_size, output_size)\n",
    "model.Init_weights()  # Initialize the weights\n",
    "\n",
    "loss_fn = nn.MSELoss()  \n",
    "\n",
    "# Train the network using DFA and print XOR accuracy\n",
    "train(model, data, targets, loss_fn, learning_rate=0.1, epochs=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
