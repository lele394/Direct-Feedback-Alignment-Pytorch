{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couldn't fix issues with matrices dimensions, might be easy, but really I can't find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 15:13:10.035074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 15:13:10.035776: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-06 15:13:10.040242: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-06 15:13:10.051786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 15:13:10.073569: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 15:13:10.079557: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 15:13:10.095489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 15:13:11.036989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last working DFA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class DFAModel(tf.keras.Model):\n",
    "    def __init__(self, output_size):\n",
    "        super(DFAModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(50, activation='tanh')\n",
    "        self.dense2 = layers.Dense(50, activation='tanh')\n",
    "        self.dense3 = layers.Dense(50, activation='tanh')\n",
    "        self.output_layer = layers.Dense(output_size)\n",
    "        \n",
    "        # Random feedback weights for DFA\n",
    "        # self.feedback1 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "        # self.feedback2 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "        # self.feedback3 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "\n",
    "        self.feedback_mat = [\n",
    "             tf.random.normal((output_size,50), dtype=tf.float32) for _ in range(3) # (2,50) for dimension matching\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, inputs, no_feedback):\n",
    "        # Forward pass\n",
    "        a1 = self.dense1(inputs)  # Pre-activation output of layer 1\n",
    "        h1 = tf.nn.tanh(a1)  # Activation output of layer 1\n",
    "        \n",
    "        a2 = self.dense2(h1)  # Pre-activation output of layer 2\n",
    "        h2 = tf.nn.tanh(a2)  # Activation output of layer 2\n",
    "        \n",
    "        a3 = self.dense3(h2)  # Pre-activation output of layer 3\n",
    "        h3 = tf.nn.tanh(a3)  # Activation output of layer 3\n",
    "        \n",
    "        ANN_output = self.output_layer(h3)  # Final output\n",
    "\n",
    "\n",
    "\n",
    "        if no_feedback:return ANN_output # Use this when dealing with backprop\n",
    "        else:return ANN_output, a1, a2, a3, h1, h2, h3  # Return output and pre-activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anas like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class DFAModel_anas(tf.keras.Model):\n",
    "    def __init__(self, output_size):\n",
    "        super(DFAModel_anas, self).__init__()\n",
    "        self.dense1 = layers.Dense(2, activation='tanh')\n",
    "        self.dense2 = layers.Dense(40, activation='tanh')\n",
    "        self.output_layer = layers.Dense(output_size)\n",
    "        \n",
    "        # Random feedback weights for DFA\n",
    "        # self.feedback1 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "        # self.feedback2 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "        # self.feedback3 = tf.random.normal((50, 2), dtype=tf.float32)  \n",
    "\n",
    "        self.feedback_mat = [\n",
    "             tf.random.normal((output_size, 2), dtype=tf.float32),\n",
    "             tf.random.normal((output_size,40), dtype=tf.float32)\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, inputs, no_feedback):\n",
    "        # Forward pass\n",
    "        a1 = self.dense1(inputs)  # Pre-activation output of layer 1\n",
    "        h1 = tf.nn.tanh(a1)  # Activation output of layer 1\n",
    "        \n",
    "        a2 = self.dense2(h1)  # Pre-activation output of layer 2\n",
    "        h2 = tf.nn.tanh(a2)  # Activation output of layer 2\n",
    "        \n",
    "        ANN_output = self.output_layer(h2)  # Final output\n",
    "\n",
    "\n",
    "\n",
    "        if no_feedback:return ANN_output # Use this when dealing with backprop\n",
    "        else:return ANN_output, a1, a2,  h1, h2 # Return output and pre-activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFA model support for different input, hidden and output layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFAModel_dynamic(tf.keras.Model):\n",
    "    def __init__(self, input_size, hidden_number, hidden_sizes, output_size, act_fn='tanh'):\n",
    "        super(DFAModel_dynamic, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = layers.Dense(input_size, activation=act_fn)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(hidden_number):\n",
    "            self.hidden_layers.append(layers.Dense(hidden_sizes, activation=act_fn))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = layers.Dense(output_size)\n",
    "\n",
    "        # Feedback matrices for DFA\n",
    "        self.feedback_mat = [\n",
    "            tf.random.normal((output_size, input_size), dtype=tf.float32)\n",
    "        ] + [\n",
    "            tf.random.normal((output_size, hidden_sizes), dtype=tf.float32) for _ in range( hidden_number)\n",
    "        ]\n",
    "\n",
    "\n",
    "        # print(self.feedback_mat)\n",
    "\n",
    "    def call(self, inputs, no_feedback):\n",
    "        # Forward pass through input layer\n",
    "        x = self.input_layer(inputs)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        hidden_activations = []\n",
    "        for layer in self.hidden_layers:\n",
    "            x = tf.nn.tanh(layer(x))  # Activation output of hidden layer\n",
    "            hidden_activations.append(x)\n",
    "        \n",
    "        # Final output layer\n",
    "        ANN_output = self.output_layer(x)\n",
    "\n",
    "        if no_feedback:\n",
    "            return ANN_output  # Use this when dealing with backprop\n",
    "        else:\n",
    "            return ANN_output, hidden_activations  # Return output and activations for DFA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom DFA weight update function\n",
    "def custom_backprop(model, inputs, targets, learning_rate=0.01):\n",
    "\n",
    "    # CURRENTLY NOT DFA BUT BACKPROP FOR TESTING PURPOSES\n",
    "\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs, no_feedback=True)  # Forward pass through the entire model\n",
    "        loss = loss_fn(targets, outputs)  # Compute the loss\n",
    "\n",
    "    # Compute gradients for all trainable variables in the model\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Update weights for all layers using backpropagation\n",
    "    for var, grad in zip(model.trainable_variables, gradients):\n",
    "        var.assign_sub(learning_rate * grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working Custom DFA for 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom DFA weight update function\n",
    "def custom_dfa_training_step(model, inputs, targets, learning_rate=0.01):\n",
    "    \n",
    "    # Get batch size\n",
    "    batch_size = inputs.shape[0]\n",
    "    \n",
    "    # 1. Forward pass to get the output from the model\n",
    "    ANN_output, a1, a2, a3, h1, h2, h3 = model(inputs, no_feedback=False)\n",
    "    \n",
    "    # 2. Define the error\n",
    "    error = ANN_output - targets  # Error as defined: ANN_output - targets\n",
    "\n",
    "\n",
    "    # 3. Define random matrices for weight updates (feedback)\n",
    "    # They are initialized in the model\n",
    "\n",
    "    # 4. Update weights for each layer\n",
    "\n",
    "    def ComputeLayerDeltaW(i_layer, a, error, h):\n",
    "        B = model.feedback_mat[i_layer] # Random B matrix\n",
    "        d_activation = tanh_derivative(a)  # F' \n",
    "        delta_a = np.dot(error, B) * d_activation  # delta_a = (B.e)xF'(a)\n",
    "        # Convert delta_a1 to float32 if necessary\n",
    "        delta_a = tf.convert_to_tensor(delta_a, dtype=tf.float32)\n",
    "\n",
    "        # Normalize by batch size\n",
    "        delta_a /= batch_size  # Avoid exponential growth\n",
    "\n",
    "        delta_a_transpose = tf.transpose(delta_a)\n",
    "        delta_W = -tf.matmul(delta_a_transpose, h) # delta_w = -delta_a.h\n",
    "\n",
    "        return delta_W \n",
    "\n",
    "    # 4.2\n",
    "    delta_W1 = ComputeLayerDeltaW(0, a1, error, inputs)\n",
    "    delta_W2 = ComputeLayerDeltaW(1, a2, error, h1)\n",
    "    delta_W3 = ComputeLayerDeltaW(2, a3, error, h2)\n",
    "\n",
    "    # 4.3 assigning weights\n",
    "    weights1 = model.dense1.kernel \n",
    "    delta_W1 = learning_rate * delta_W1  # Clip the gradient updates\n",
    "    weights1.assign(weights1 + tf.transpose(delta_W1))\n",
    "\n",
    "    weights2 = model.dense2.kernel  \n",
    "    delta_W2 = learning_rate * delta_W2  # Clip the gradient updates\n",
    "    weights2.assign(weights2 + tf.transpose(delta_W2))\n",
    "\n",
    "    weights3 = model.dense3.kernel  \n",
    "    delta_W3 = learning_rate * delta_W3  # Clip the gradient updates\n",
    "    weights3.assign(weights3 + tf.transpose(delta_W3))\n",
    "\n",
    "    # Update for the output layer\n",
    "    output_weights = model.output_layer.kernel  # Get the weights of the output layer\n",
    "    delta_output_weights = -tf.matmul(tf.transpose(error), h3)   # Compute the update for output layer\n",
    "    output_weights.assign(output_weights + tf.transpose(delta_output_weights))  # Update the output layer weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anas like DFA for 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom DFA weight update function\n",
    "def anas_dfa_training_step(model, inputs, targets, learning_rate=0.01):\n",
    "    \n",
    "    # Get batch size\n",
    "    batch_size = inputs.shape[0]\n",
    "    \n",
    "    # 1. Forward pass to get the output from the model\n",
    "    ANN_output, a1, a2, h1, h2 = model(inputs, no_feedback=False)\n",
    "    \n",
    "    # 2. Define the error\n",
    "    error = ANN_output - targets  # Error as defined: ANN_output - targets\n",
    "\n",
    "\n",
    "    # 3. Define random matrices for weight updates (feedback)\n",
    "    # They are initialized in the model\n",
    "\n",
    "    # 4. Update weights for each layer\n",
    "\n",
    "    def ComputeLayerDeltaW(i_layer, a, error, h):\n",
    "        B = model.feedback_mat[i_layer] # Random B matrix\n",
    "        d_activation = tanh_derivative(a)  # F' \n",
    "        delta_a = np.dot(error, B) * d_activation  # delta_a = (B.e)xF'(a)\n",
    "        # Convert delta_a1 to float32 if necessary\n",
    "        delta_a = tf.convert_to_tensor(delta_a, dtype=tf.float32)\n",
    "\n",
    "        # Normalize by batch size\n",
    "        delta_a /= batch_size  # Avoid exponential growth\n",
    "\n",
    "        delta_a_transpose = tf.transpose(delta_a)\n",
    "        delta_W = -tf.matmul(delta_a_transpose, h) # delta_w = -delta_a.h\n",
    "\n",
    "        return delta_W \n",
    "\n",
    "    # 4.2\n",
    "    delta_W1 = ComputeLayerDeltaW(0, a1, error, inputs)\n",
    "    delta_W2 = ComputeLayerDeltaW(1, a2, error, h1)\n",
    "\n",
    "    # 4.3 assigning weights\n",
    "    weights1 = model.dense1.kernel \n",
    "    delta_W1 = learning_rate * delta_W1  # Clip the gradient updates\n",
    "    weights1.assign(weights1 + tf.transpose(delta_W1))\n",
    "\n",
    "    weights2 = model.dense2.kernel  \n",
    "    delta_W2 = learning_rate * delta_W2  # Clip the gradient updates\n",
    "    weights2.assign(weights2 + tf.transpose(delta_W2))\n",
    "\n",
    "\n",
    "    # Update for the output layer\n",
    "    output_weights = model.output_layer.kernel  # Get the weights of the output layer\n",
    "    delta_output_weights = -tf.matmul(tf.transpose(error), h2)   # Compute the update for output layer\n",
    "    output_weights.assign(output_weights + tf.transpose(delta_output_weights))  # Update the output layer weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DFA working with dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa_dynamic(model, inputs, targets, learning_rate=0.01):\n",
    "    # Get batch size\n",
    "    batch_size = inputs.shape[0]\n",
    "    \n",
    "    # 1. Forward pass to get the output and hidden activations from the model\n",
    "    ANN_output, hidden_activations = model(inputs, no_feedback=False)\n",
    "    \n",
    "    # 2. Define the error\n",
    "    error = ANN_output - targets  # Error as defined: ANN_output - targets\n",
    "    \n",
    "    # 3. Update weights for each hidden layer\n",
    "    def ComputeLayerDeltaW(i_layer, activation, error, inputs_or_hidden):\n",
    "        B = model.feedback_mat[i_layer]  # Random B matrix for the layer\n",
    "        d_activation = tanh_derivative(activation)  # Derivative of the activation function\n",
    "        delta_a = tf.matmul(error, B) * d_activation  # delta_a = (B.e) * F'(activation)\n",
    "        \n",
    "        # Normalize by batch size\n",
    "        delta_a /= batch_size\n",
    "\n",
    "        # Compute weight update\n",
    "        delta_W = -tf.matmul(tf.transpose(delta_a), inputs_or_hidden)  # delta_W = -delta_a * inputs_or_hidden\n",
    "        return delta_W\n",
    "\n",
    "    # 4. Loop through each hidden layer and update weights\n",
    "    # The first layer takes the inputs directly\n",
    "    delta_W1 = ComputeLayerDeltaW(0, hidden_activations[0], error, inputs)\n",
    "    \n",
    "    # Assign weights for the first layer\n",
    "    weights1 = model.input_layer.kernel\n",
    "    delta_W1 = learning_rate * delta_W1\n",
    "    weights1.assign(tf.transpose(delta_W1))\n",
    "\n",
    "    # Handle all hidden layers\n",
    "    for i in range(1, len(hidden_activations)):\n",
    "        print(i)\n",
    "        delta_W_hidden = ComputeLayerDeltaW(i, hidden_activations[i], error, hidden_activations[i-1])\n",
    "        \n",
    "        # Assign weights for the hidden layer\n",
    "        weights_hidden = model.hidden_layers[i-1].kernel\n",
    "        delta_W_hidden = learning_rate * delta_W_hidden\n",
    "        weights_hidden.assign(tf.transpose(delta_W_hidden))\n",
    "    \n",
    "    # 5. Update the output layer\n",
    "    output_weights = model.output_layer.kernel  # Get the weights of the output layer\n",
    "    delta_output_weights = -tf.matmul(tf.transpose(error), hidden_activations[-1])  # delta_output_weights = -error * last hidden activation\n",
    "    delta_output_weights /= batch_size  # Normalize by batch size\n",
    "    output_weights.assign(tf.transpose(delta_output_weights))  # Update the output layer weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"dfa_model_anas\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"dfa_model_anas\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m40\u001b[0m)                │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m41\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167</span> (668.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m167\u001b[0m (668.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167</span> (668.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m167\u001b[0m (668.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh: 0\n",
      "epcoh: 1\n",
      "epcoh: 2\n",
      "epcoh: 3\n",
      "epcoh: 4\n",
      "epcoh: 5\n",
      "epcoh: 6\n",
      "epcoh: 7\n",
      "epcoh: 8\n",
      "epcoh: 9\n",
      "epcoh: 10\n",
      "epcoh: 11\n",
      "epcoh: 12\n",
      "epcoh: 13\n",
      "epcoh: 14\n",
      "epcoh: 15\n",
      "epcoh: 16\n",
      "epcoh: 17\n",
      "epcoh: 18\n",
      "epcoh: 19\n",
      "epcoh: 20\n",
      "epcoh: 21\n",
      "epcoh: 22\n",
      "epcoh: 23\n",
      "epcoh: 24\n",
      "epcoh: 25\n",
      "epcoh: 26\n",
      "epcoh: 27\n",
      "epcoh: 28\n",
      "epcoh: 29\n",
      "epcoh: 30\n",
      "epcoh: 31\n",
      "epcoh: 32\n",
      "epcoh: 33\n",
      "epcoh: 34\n",
      "epcoh: 35\n",
      "epcoh: 36\n",
      "epcoh: 37\n",
      "epcoh: 38\n",
      "epcoh: 39\n",
      "epcoh: 40\n",
      "epcoh: 41\n",
      "epcoh: 42\n",
      "epcoh: 43\n",
      "epcoh: 44\n",
      "epcoh: 45\n",
      "epcoh: 46\n",
      "epcoh: 47\n",
      "epcoh: 48\n",
      "epcoh: 49\n",
      "epcoh: 50\n",
      "epcoh: 51\n",
      "epcoh: 52\n",
      "epcoh: 53\n",
      "epcoh: 54\n",
      "epcoh: 55\n",
      "epcoh: 56\n",
      "epcoh: 57\n",
      "epcoh: 58\n",
      "epcoh: 59\n",
      "epcoh: 60\n",
      "epcoh: 61\n",
      "epcoh: 62\n",
      "epcoh: 63\n",
      "epcoh: 64\n",
      "epcoh: 65\n",
      "epcoh: 66\n",
      "epcoh: 67\n",
      "epcoh: 68\n",
      "epcoh: 69\n",
      "epcoh: 70\n",
      "epcoh: 71\n",
      "epcoh: 72\n",
      "epcoh: 73\n",
      "epcoh: 74\n",
      "epcoh: 75\n",
      "epcoh: 76\n",
      "epcoh: 77\n",
      "epcoh: 78\n",
      "epcoh: 79\n",
      "epcoh: 80\n",
      "epcoh: 81\n",
      "epcoh: 82\n",
      "epcoh: 83\n",
      "epcoh: 84\n",
      "epcoh: 85\n",
      "epcoh: 86\n",
      "epcoh: 87\n",
      "epcoh: 88\n",
      "epcoh: 89\n",
      "epcoh: 90\n",
      "epcoh: 91\n",
      "epcoh: 92\n",
      "epcoh: 93\n",
      "epcoh: 94\n",
      "epcoh: 95\n",
      "epcoh: 96\n",
      "epcoh: 97\n",
      "epcoh: 98\n",
      "epcoh: 99\n",
      "epcoh: 100\n",
      "epcoh: 101\n",
      "epcoh: 102\n",
      "epcoh: 103\n",
      "epcoh: 104\n",
      "epcoh: 105\n",
      "epcoh: 106\n",
      "epcoh: 107\n",
      "epcoh: 108\n",
      "epcoh: 109\n",
      "epcoh: 110\n",
      "epcoh: 111\n",
      "epcoh: 112\n",
      "epcoh: 113\n",
      "epcoh: 114\n",
      "epcoh: 115\n",
      "epcoh: 116\n",
      "epcoh: 117\n",
      "epcoh: 118\n",
      "epcoh: 119\n",
      "epcoh: 120\n",
      "epcoh: 121\n",
      "epcoh: 122\n",
      "epcoh: 123\n",
      "epcoh: 124\n",
      "epcoh: 125\n",
      "epcoh: 126\n",
      "epcoh: 127\n",
      "epcoh: 128\n",
      "epcoh: 129\n",
      "epcoh: 130\n",
      "epcoh: 131\n",
      "epcoh: 132\n",
      "epcoh: 133\n",
      "epcoh: 134\n",
      "epcoh: 135\n",
      "epcoh: 136\n",
      "epcoh: 137\n",
      "epcoh: 138\n",
      "epcoh: 139\n",
      "epcoh: 140\n",
      "epcoh: 141\n",
      "epcoh: 142\n",
      "epcoh: 143\n",
      "epcoh: 144\n",
      "epcoh: 145\n",
      "epcoh: 146\n",
      "epcoh: 147\n",
      "epcoh: 148\n",
      "epcoh: 149\n",
      "epcoh: 150\n",
      "epcoh: 151\n",
      "epcoh: 152\n",
      "epcoh: 153\n",
      "epcoh: 154\n",
      "epcoh: 155\n",
      "epcoh: 156\n",
      "epcoh: 157\n",
      "epcoh: 158\n",
      "epcoh: 159\n",
      "epcoh: 160\n",
      "epcoh: 161\n",
      "epcoh: 162\n",
      "epcoh: 163\n",
      "epcoh: 164\n",
      "epcoh: 165\n",
      "epcoh: 166\n",
      "epcoh: 167\n",
      "epcoh: 168\n",
      "epcoh: 169\n",
      "epcoh: 170\n",
      "epcoh: 171\n",
      "epcoh: 172\n",
      "epcoh: 173\n",
      "epcoh: 174\n",
      "epcoh: 175\n",
      "epcoh: 176\n",
      "epcoh: 177\n",
      "epcoh: 178\n",
      "epcoh: 179\n",
      "epcoh: 180\n",
      "epcoh: 181\n",
      "epcoh: 182\n",
      "epcoh: 183\n",
      "epcoh: 184\n",
      "epcoh: 185\n",
      "epcoh: 186\n",
      "epcoh: 187\n",
      "epcoh: 188\n",
      "epcoh: 189\n",
      "epcoh: 190\n",
      "epcoh: 191\n",
      "epcoh: 192\n",
      "epcoh: 193\n",
      "epcoh: 194\n",
      "epcoh: 195\n",
      "epcoh: 196\n",
      "epcoh: 197\n",
      "epcoh: 198\n",
      "epcoh: 199\n",
      "epcoh: 200\n",
      "epcoh: 201\n",
      "epcoh: 202\n",
      "epcoh: 203\n",
      "epcoh: 204\n",
      "epcoh: 205\n",
      "epcoh: 206\n",
      "epcoh: 207\n",
      "epcoh: 208\n",
      "epcoh: 209\n",
      "epcoh: 210\n",
      "epcoh: 211\n",
      "epcoh: 212\n",
      "epcoh: 213\n",
      "epcoh: 214\n",
      "epcoh: 215\n",
      "epcoh: 216\n",
      "epcoh: 217\n",
      "epcoh: 218\n",
      "epcoh: 219\n",
      "epcoh: 220\n",
      "epcoh: 221\n",
      "epcoh: 222\n",
      "epcoh: 223\n",
      "epcoh: 224\n",
      "epcoh: 225\n",
      "epcoh: 226\n",
      "epcoh: 227\n",
      "epcoh: 228\n",
      "epcoh: 229\n",
      "epcoh: 230\n",
      "epcoh: 231\n",
      "epcoh: 232\n",
      "epcoh: 233\n",
      "epcoh: 234\n",
      "epcoh: 235\n",
      "epcoh: 236\n",
      "epcoh: 237\n",
      "epcoh: 238\n",
      "epcoh: 239\n",
      "epcoh: 240\n",
      "epcoh: 241\n",
      "epcoh: 242\n",
      "epcoh: 243\n",
      "epcoh: 244\n",
      "epcoh: 245\n",
      "epcoh: 246\n",
      "epcoh: 247\n",
      "epcoh: 248\n",
      "epcoh: 249\n",
      "epcoh: 250\n",
      "epcoh: 251\n",
      "epcoh: 252\n",
      "epcoh: 253\n",
      "epcoh: 254\n",
      "epcoh: 255\n",
      "epcoh: 256\n",
      "epcoh: 257\n",
      "epcoh: 258\n",
      "epcoh: 259\n",
      "epcoh: 260\n",
      "epcoh: 261\n",
      "epcoh: 262\n",
      "epcoh: 263\n",
      "epcoh: 264\n",
      "epcoh: 265\n",
      "epcoh: 266\n",
      "epcoh: 267\n",
      "epcoh: 268\n",
      "epcoh: 269\n",
      "epcoh: 270\n",
      "epcoh: 271\n",
      "epcoh: 272\n",
      "epcoh: 273\n",
      "epcoh: 274\n",
      "epcoh: 275\n",
      "epcoh: 276\n",
      "epcoh: 277\n",
      "epcoh: 278\n",
      "epcoh: 279\n",
      "epcoh: 280\n",
      "epcoh: 281\n",
      "epcoh: 282\n",
      "epcoh: 283\n",
      "epcoh: 284\n",
      "epcoh: 285\n",
      "epcoh: 286\n",
      "epcoh: 287\n",
      "epcoh: 288\n",
      "epcoh: 289\n",
      "epcoh: 290\n",
      "epcoh: 291\n",
      "epcoh: 292\n",
      "epcoh: 293\n",
      "epcoh: 294\n",
      "epcoh: 295\n",
      "epcoh: 296\n",
      "epcoh: 297\n",
      "epcoh: 298\n",
      "epcoh: 299\n",
      "epcoh: 300\n",
      "epcoh: 301\n",
      "epcoh: 302\n",
      "epcoh: 303\n",
      "epcoh: 304\n",
      "epcoh: 305\n",
      "epcoh: 306\n",
      "epcoh: 307\n",
      "epcoh: 308\n",
      "epcoh: 309\n",
      "epcoh: 310\n",
      "epcoh: 311\n",
      "epcoh: 312\n",
      "epcoh: 313\n",
      "epcoh: 314\n",
      "epcoh: 315\n",
      "epcoh: 316\n",
      "epcoh: 317\n",
      "epcoh: 318\n",
      "epcoh: 319\n",
      "epcoh: 320\n",
      "epcoh: 321\n",
      "epcoh: 322\n",
      "epcoh: 323\n",
      "epcoh: 324\n",
      "epcoh: 325\n",
      "epcoh: 326\n",
      "epcoh: 327\n",
      "epcoh: 328\n",
      "epcoh: 329\n",
      "epcoh: 330\n",
      "epcoh: 331\n",
      "epcoh: 332\n",
      "epcoh: 333\n",
      "epcoh: 334\n",
      "epcoh: 335\n",
      "epcoh: 336\n",
      "epcoh: 337\n",
      "epcoh: 338\n",
      "epcoh: 339\n",
      "epcoh: 340\n",
      "epcoh: 341\n",
      "epcoh: 342\n",
      "epcoh: 343\n",
      "epcoh: 344\n",
      "epcoh: 345\n",
      "epcoh: 346\n",
      "epcoh: 347\n",
      "epcoh: 348\n",
      "epcoh: 349\n",
      "epcoh: 350\n",
      "epcoh: 351\n",
      "epcoh: 352\n",
      "epcoh: 353\n",
      "epcoh: 354\n",
      "epcoh: 355\n",
      "epcoh: 356\n",
      "epcoh: 357\n",
      "epcoh: 358\n",
      "epcoh: 359\n",
      "epcoh: 360\n",
      "epcoh: 361\n",
      "epcoh: 362\n",
      "epcoh: 363\n",
      "epcoh: 364\n",
      "epcoh: 365\n",
      "epcoh: 366\n",
      "epcoh: 367\n",
      "epcoh: 368\n",
      "epcoh: 369\n",
      "epcoh: 370\n",
      "epcoh: 371\n",
      "epcoh: 372\n",
      "epcoh: 373\n",
      "epcoh: 374\n",
      "epcoh: 375\n",
      "epcoh: 376\n",
      "epcoh: 377\n",
      "epcoh: 378\n",
      "epcoh: 379\n",
      "epcoh: 380\n",
      "epcoh: 381\n",
      "epcoh: 382\n",
      "epcoh: 383\n",
      "epcoh: 384\n",
      "epcoh: 385\n",
      "epcoh: 386\n",
      "epcoh: 387\n",
      "epcoh: 388\n",
      "epcoh: 389\n",
      "epcoh: 390\n",
      "epcoh: 391\n",
      "epcoh: 392\n",
      "epcoh: 393\n",
      "epcoh: 394\n",
      "epcoh: 395\n",
      "epcoh: 396\n",
      "epcoh: 397\n",
      "epcoh: 398\n",
      "epcoh: 399\n",
      "epcoh: 400\n",
      "epcoh: 401\n",
      "epcoh: 402\n",
      "epcoh: 403\n",
      "epcoh: 404\n",
      "epcoh: 405\n",
      "epcoh: 406\n",
      "epcoh: 407\n",
      "epcoh: 408\n",
      "epcoh: 409\n",
      "epcoh: 410\n",
      "epcoh: 411\n",
      "epcoh: 412\n",
      "epcoh: 413\n",
      "epcoh: 414\n",
      "epcoh: 415\n",
      "epcoh: 416\n",
      "epcoh: 417\n",
      "epcoh: 418\n",
      "epcoh: 419\n",
      "epcoh: 420\n",
      "epcoh: 421\n",
      "epcoh: 422\n",
      "epcoh: 423\n",
      "epcoh: 424\n",
      "epcoh: 425\n",
      "epcoh: 426\n",
      "epcoh: 427\n",
      "epcoh: 428\n",
      "epcoh: 429\n",
      "epcoh: 430\n",
      "epcoh: 431\n",
      "epcoh: 432\n",
      "epcoh: 433\n",
      "epcoh: 434\n",
      "epcoh: 435\n",
      "epcoh: 436\n",
      "epcoh: 437\n",
      "epcoh: 438\n",
      "epcoh: 439\n",
      "epcoh: 440\n",
      "epcoh: 441\n",
      "epcoh: 442\n",
      "epcoh: 443\n",
      "epcoh: 444\n",
      "epcoh: 445\n",
      "epcoh: 446\n",
      "epcoh: 447\n",
      "epcoh: 448\n",
      "epcoh: 449\n",
      "epcoh: 450\n",
      "epcoh: 451\n",
      "epcoh: 452\n",
      "epcoh: 453\n",
      "epcoh: 454\n",
      "epcoh: 455\n",
      "epcoh: 456\n",
      "epcoh: 457\n",
      "epcoh: 458\n",
      "epcoh: 459\n",
      "epcoh: 460\n",
      "epcoh: 461\n",
      "epcoh: 462\n",
      "epcoh: 463\n",
      "epcoh: 464\n",
      "epcoh: 465\n",
      "epcoh: 466\n",
      "epcoh: 467\n",
      "epcoh: 468\n",
      "epcoh: 469\n",
      "epcoh: 470\n",
      "epcoh: 471\n",
      "epcoh: 472\n",
      "epcoh: 473\n",
      "epcoh: 474\n",
      "epcoh: 475\n",
      "epcoh: 476\n",
      "epcoh: 477\n",
      "epcoh: 478\n",
      "epcoh: 479\n",
      "epcoh: 480\n",
      "epcoh: 481\n",
      "epcoh: 482\n",
      "epcoh: 483\n",
      "epcoh: 484\n",
      "epcoh: 485\n",
      "epcoh: 486\n",
      "epcoh: 487\n",
      "epcoh: 488\n",
      "epcoh: 489\n",
      "epcoh: 490\n",
      "epcoh: 491\n",
      "epcoh: 492\n",
      "epcoh: 493\n",
      "epcoh: 494\n",
      "epcoh: 495\n",
      "epcoh: 496\n",
      "epcoh: 497\n",
      "epcoh: 498\n",
      "epcoh: 499\n",
      "epcoh: 500\n",
      "epcoh: 501\n",
      "epcoh: 502\n",
      "epcoh: 503\n",
      "epcoh: 504\n",
      "epcoh: 505\n",
      "epcoh: 506\n",
      "epcoh: 507\n",
      "epcoh: 508\n",
      "epcoh: 509\n",
      "epcoh: 510\n",
      "epcoh: 511\n",
      "epcoh: 512\n",
      "epcoh: 513\n",
      "epcoh: 514\n",
      "epcoh: 515\n",
      "epcoh: 516\n",
      "epcoh: 517\n",
      "epcoh: 518\n",
      "epcoh: 519\n",
      "epcoh: 520\n",
      "epcoh: 521\n",
      "epcoh: 522\n",
      "epcoh: 523\n",
      "epcoh: 524\n",
      "epcoh: 525\n",
      "epcoh: 526\n",
      "epcoh: 527\n",
      "epcoh: 528\n",
      "epcoh: 529\n",
      "epcoh: 530\n",
      "epcoh: 531\n",
      "epcoh: 532\n",
      "epcoh: 533\n",
      "epcoh: 534\n",
      "epcoh: 535\n",
      "epcoh: 536\n",
      "epcoh: 537\n",
      "epcoh: 538\n",
      "epcoh: 539\n",
      "epcoh: 540\n",
      "epcoh: 541\n",
      "epcoh: 542\n",
      "epcoh: 543\n",
      "epcoh: 544\n",
      "epcoh: 545\n",
      "epcoh: 546\n",
      "epcoh: 547\n",
      "epcoh: 548\n",
      "epcoh: 549\n",
      "epcoh: 550\n",
      "epcoh: 551\n",
      "epcoh: 552\n",
      "epcoh: 553\n",
      "epcoh: 554\n",
      "epcoh: 555\n",
      "epcoh: 556\n",
      "epcoh: 557\n",
      "epcoh: 558\n",
      "epcoh: 559\n",
      "epcoh: 560\n",
      "epcoh: 561\n",
      "epcoh: 562\n",
      "epcoh: 563\n",
      "epcoh: 564\n",
      "epcoh: 565\n",
      "epcoh: 566\n",
      "epcoh: 567\n",
      "epcoh: 568\n",
      "epcoh: 569\n",
      "epcoh: 570\n",
      "epcoh: 571\n",
      "epcoh: 572\n",
      "epcoh: 573\n",
      "epcoh: 574\n",
      "epcoh: 575\n",
      "epcoh: 576\n",
      "epcoh: 577\n",
      "epcoh: 578\n",
      "epcoh: 579\n",
      "epcoh: 580\n",
      "epcoh: 581\n",
      "epcoh: 582\n",
      "epcoh: 583\n",
      "epcoh: 584\n",
      "epcoh: 585\n",
      "epcoh: 586\n",
      "epcoh: 587\n",
      "epcoh: 588\n",
      "epcoh: 589\n",
      "epcoh: 590\n",
      "epcoh: 591\n",
      "epcoh: 592\n",
      "epcoh: 593\n",
      "epcoh: 594\n",
      "epcoh: 595\n",
      "epcoh: 596\n",
      "epcoh: 597\n",
      "epcoh: 598\n",
      "epcoh: 599\n",
      "epcoh: 600\n",
      "epcoh: 601\n",
      "epcoh: 602\n",
      "epcoh: 603\n",
      "epcoh: 604\n",
      "epcoh: 605\n",
      "epcoh: 606\n",
      "epcoh: 607\n",
      "epcoh: 608\n",
      "epcoh: 609\n",
      "epcoh: 610\n",
      "epcoh: 611\n",
      "epcoh: 612\n",
      "epcoh: 613\n",
      "epcoh: 614\n",
      "epcoh: 615\n",
      "epcoh: 616\n",
      "epcoh: 617\n",
      "epcoh: 618\n",
      "epcoh: 619\n",
      "epcoh: 620\n",
      "epcoh: 621\n",
      "epcoh: 622\n",
      "epcoh: 623\n",
      "epcoh: 624\n",
      "epcoh: 625\n",
      "epcoh: 626\n",
      "epcoh: 627\n",
      "epcoh: 628\n",
      "epcoh: 629\n",
      "epcoh: 630\n",
      "epcoh: 631\n",
      "epcoh: 632\n",
      "epcoh: 633\n",
      "epcoh: 634\n",
      "epcoh: 635\n",
      "epcoh: 636\n",
      "epcoh: 637\n",
      "epcoh: 638\n",
      "epcoh: 639\n",
      "epcoh: 640\n",
      "epcoh: 641\n",
      "epcoh: 642\n",
      "epcoh: 643\n",
      "epcoh: 644\n",
      "epcoh: 645\n",
      "epcoh: 646\n",
      "epcoh: 647\n",
      "epcoh: 648\n",
      "epcoh: 649\n",
      "epcoh: 650\n",
      "epcoh: 651\n",
      "epcoh: 652\n",
      "epcoh: 653\n",
      "epcoh: 654\n",
      "epcoh: 655\n",
      "epcoh: 656\n",
      "epcoh: 657\n",
      "epcoh: 658\n",
      "epcoh: 659\n",
      "epcoh: 660\n",
      "epcoh: 661\n",
      "epcoh: 662\n",
      "epcoh: 663\n",
      "epcoh: 664\n",
      "epcoh: 665\n",
      "epcoh: 666\n",
      "epcoh: 667\n",
      "epcoh: 668\n",
      "epcoh: 669\n",
      "epcoh: 670\n",
      "epcoh: 671\n",
      "epcoh: 672\n",
      "epcoh: 673\n",
      "epcoh: 674\n",
      "epcoh: 675\n",
      "epcoh: 676\n",
      "epcoh: 677\n",
      "epcoh: 678\n",
      "epcoh: 679\n",
      "epcoh: 680\n",
      "epcoh: 681\n",
      "epcoh: 682\n",
      "epcoh: 683\n",
      "epcoh: 684\n",
      "epcoh: 685\n",
      "epcoh: 686\n",
      "epcoh: 687\n",
      "epcoh: 688\n",
      "epcoh: 689\n",
      "epcoh: 690\n",
      "epcoh: 691\n",
      "epcoh: 692\n",
      "epcoh: 693\n",
      "epcoh: 694\n",
      "epcoh: 695\n",
      "epcoh: 696\n",
      "epcoh: 697\n",
      "epcoh: 698\n",
      "epcoh: 699\n",
      "epcoh: 700\n",
      "epcoh: 701\n",
      "epcoh: 702\n",
      "epcoh: 703\n",
      "epcoh: 704\n",
      "epcoh: 705\n",
      "epcoh: 706\n",
      "epcoh: 707\n",
      "epcoh: 708\n",
      "epcoh: 709\n",
      "epcoh: 710\n",
      "epcoh: 711\n",
      "epcoh: 712\n",
      "epcoh: 713\n",
      "epcoh: 714\n",
      "epcoh: 715\n",
      "epcoh: 716\n",
      "epcoh: 717\n",
      "epcoh: 718\n",
      "epcoh: 719\n",
      "epcoh: 720\n",
      "epcoh: 721\n",
      "epcoh: 722\n",
      "epcoh: 723\n",
      "epcoh: 724\n",
      "epcoh: 725\n",
      "epcoh: 726\n",
      "epcoh: 727\n",
      "epcoh: 728\n",
      "epcoh: 729\n",
      "epcoh: 730\n",
      "epcoh: 731\n",
      "epcoh: 732\n",
      "epcoh: 733\n",
      "epcoh: 734\n",
      "epcoh: 735\n",
      "epcoh: 736\n",
      "epcoh: 737\n",
      "epcoh: 738\n",
      "epcoh: 739\n",
      "epcoh: 740\n",
      "epcoh: 741\n",
      "epcoh: 742\n",
      "epcoh: 743\n",
      "epcoh: 744\n",
      "epcoh: 745\n",
      "epcoh: 746\n",
      "epcoh: 747\n",
      "epcoh: 748\n",
      "epcoh: 749\n",
      "epcoh: 750\n",
      "epcoh: 751\n",
      "epcoh: 752\n",
      "epcoh: 753\n",
      "epcoh: 754\n",
      "epcoh: 755\n",
      "epcoh: 756\n",
      "epcoh: 757\n",
      "epcoh: 758\n",
      "epcoh: 759\n",
      "epcoh: 760\n",
      "epcoh: 761\n",
      "epcoh: 762\n",
      "epcoh: 763\n",
      "epcoh: 764\n",
      "epcoh: 765\n",
      "epcoh: 766\n",
      "epcoh: 767\n",
      "epcoh: 768\n",
      "epcoh: 769\n",
      "epcoh: 770\n",
      "epcoh: 771\n",
      "epcoh: 772\n",
      "epcoh: 773\n",
      "epcoh: 774\n",
      "epcoh: 775\n",
      "epcoh: 776\n",
      "epcoh: 777\n",
      "epcoh: 778\n",
      "epcoh: 779\n",
      "epcoh: 780\n",
      "epcoh: 781\n",
      "epcoh: 782\n",
      "epcoh: 783\n",
      "epcoh: 784\n",
      "epcoh: 785\n",
      "epcoh: 786\n",
      "epcoh: 787\n",
      "epcoh: 788\n",
      "epcoh: 789\n",
      "epcoh: 790\n",
      "epcoh: 791\n",
      "epcoh: 792\n",
      "epcoh: 793\n",
      "epcoh: 794\n",
      "epcoh: 795\n",
      "epcoh: 796\n",
      "epcoh: 797\n",
      "epcoh: 798\n",
      "epcoh: 799\n",
      "epcoh: 800\n",
      "epcoh: 801\n",
      "epcoh: 802\n",
      "epcoh: 803\n",
      "epcoh: 804\n",
      "epcoh: 805\n",
      "epcoh: 806\n",
      "epcoh: 807\n",
      "epcoh: 808\n",
      "epcoh: 809\n",
      "epcoh: 810\n",
      "epcoh: 811\n",
      "epcoh: 812\n",
      "epcoh: 813\n",
      "epcoh: 814\n",
      "epcoh: 815\n",
      "epcoh: 816\n",
      "epcoh: 817\n",
      "epcoh: 818\n",
      "epcoh: 819\n",
      "epcoh: 820\n",
      "epcoh: 821\n",
      "epcoh: 822\n",
      "epcoh: 823\n",
      "epcoh: 824\n",
      "epcoh: 825\n",
      "epcoh: 826\n",
      "epcoh: 827\n",
      "epcoh: 828\n",
      "epcoh: 829\n",
      "epcoh: 830\n",
      "epcoh: 831\n",
      "epcoh: 832\n",
      "epcoh: 833\n",
      "epcoh: 834\n",
      "epcoh: 835\n",
      "epcoh: 836\n",
      "epcoh: 837\n",
      "epcoh: 838\n",
      "epcoh: 839\n",
      "epcoh: 840\n",
      "epcoh: 841\n",
      "epcoh: 842\n",
      "epcoh: 843\n",
      "epcoh: 844\n",
      "epcoh: 845\n",
      "epcoh: 846\n",
      "epcoh: 847\n",
      "epcoh: 848\n",
      "epcoh: 849\n",
      "epcoh: 850\n",
      "epcoh: 851\n",
      "epcoh: 852\n",
      "epcoh: 853\n",
      "epcoh: 854\n",
      "epcoh: 855\n",
      "epcoh: 856\n",
      "epcoh: 857\n",
      "epcoh: 858\n",
      "epcoh: 859\n",
      "epcoh: 860\n",
      "epcoh: 861\n",
      "epcoh: 862\n",
      "epcoh: 863\n",
      "epcoh: 864\n",
      "epcoh: 865\n",
      "epcoh: 866\n",
      "epcoh: 867\n",
      "epcoh: 868\n",
      "epcoh: 869\n",
      "epcoh: 870\n",
      "epcoh: 871\n",
      "epcoh: 872\n",
      "epcoh: 873\n",
      "epcoh: 874\n",
      "epcoh: 875\n",
      "epcoh: 876\n",
      "epcoh: 877\n",
      "epcoh: 878\n",
      "epcoh: 879\n",
      "epcoh: 880\n",
      "epcoh: 881\n",
      "epcoh: 882\n",
      "epcoh: 883\n",
      "epcoh: 884\n",
      "epcoh: 885\n",
      "epcoh: 886\n",
      "epcoh: 887\n",
      "epcoh: 888\n",
      "epcoh: 889\n",
      "epcoh: 890\n",
      "epcoh: 891\n",
      "epcoh: 892\n",
      "epcoh: 893\n",
      "epcoh: 894\n",
      "epcoh: 895\n",
      "epcoh: 896\n",
      "epcoh: 897\n",
      "epcoh: 898\n",
      "epcoh: 899\n",
      "epcoh: 900\n",
      "epcoh: 901\n",
      "epcoh: 902\n",
      "epcoh: 903\n",
      "epcoh: 904\n",
      "epcoh: 905\n",
      "epcoh: 906\n",
      "epcoh: 907\n",
      "epcoh: 908\n",
      "epcoh: 909\n",
      "epcoh: 910\n",
      "epcoh: 911\n",
      "epcoh: 912\n",
      "epcoh: 913\n",
      "epcoh: 914\n",
      "epcoh: 915\n",
      "epcoh: 916\n",
      "epcoh: 917\n",
      "epcoh: 918\n",
      "epcoh: 919\n",
      "epcoh: 920\n",
      "epcoh: 921\n",
      "epcoh: 922\n",
      "epcoh: 923\n",
      "epcoh: 924\n",
      "epcoh: 925\n",
      "epcoh: 926\n",
      "epcoh: 927\n",
      "epcoh: 928\n",
      "epcoh: 929\n",
      "epcoh: 930\n",
      "epcoh: 931\n",
      "epcoh: 932\n",
      "epcoh: 933\n",
      "epcoh: 934\n",
      "epcoh: 935\n",
      "epcoh: 936\n",
      "epcoh: 937\n",
      "epcoh: 938\n",
      "epcoh: 939\n",
      "epcoh: 940\n",
      "epcoh: 941\n",
      "epcoh: 942\n",
      "epcoh: 943\n",
      "epcoh: 944\n",
      "epcoh: 945\n",
      "epcoh: 946\n",
      "epcoh: 947\n",
      "epcoh: 948\n",
      "epcoh: 949\n",
      "epcoh: 950\n",
      "epcoh: 951\n",
      "epcoh: 952\n",
      "epcoh: 953\n",
      "epcoh: 954\n",
      "epcoh: 955\n",
      "epcoh: 956\n",
      "epcoh: 957\n",
      "epcoh: 958\n",
      "epcoh: 959\n",
      "epcoh: 960\n",
      "epcoh: 961\n",
      "epcoh: 962\n",
      "epcoh: 963\n",
      "epcoh: 964\n",
      "epcoh: 965\n",
      "epcoh: 966\n",
      "epcoh: 967\n",
      "epcoh: 968\n",
      "epcoh: 969\n",
      "epcoh: 970\n",
      "epcoh: 971\n",
      "epcoh: 972\n",
      "epcoh: 973\n",
      "epcoh: 974\n",
      "epcoh: 975\n",
      "epcoh: 976\n",
      "epcoh: 977\n",
      "epcoh: 978\n",
      "epcoh: 979\n",
      "epcoh: 980\n",
      "epcoh: 981\n",
      "epcoh: 982\n",
      "epcoh: 983\n",
      "epcoh: 984\n",
      "epcoh: 985\n",
      "epcoh: 986\n",
      "epcoh: 987\n",
      "epcoh: 988\n",
      "epcoh: 989\n",
      "epcoh: 990\n",
      "epcoh: 991\n",
      "epcoh: 992\n",
      "epcoh: 993\n",
      "epcoh: 994\n",
      "epcoh: 995\n",
      "epcoh: 996\n",
      "epcoh: 997\n",
      "epcoh: 998\n",
      "epcoh: 999\n",
      "[0 0] tf.Tensor([nan], shape=(1,), dtype=float32)\n",
      "[0 1] tf.Tensor([nan], shape=(1,), dtype=float32)\n",
      "[1 0] tf.Tensor([nan], shape=(1,), dtype=float32)\n",
      "[1 1] tf.Tensor([nan], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dfa_step = custom_dfa_training_step\n",
    "# dfa_model = DFAModel(1)\n",
    "\n",
    "dfa_step = anas_dfa_training_step\n",
    "dfa_model = DFAModel_anas(1)\n",
    "\n",
    "# dfa_step = dfa_dynamic\n",
    "# dfa_model = DFAModel_dynamic(2, 1, 40, 1, 'tanh')\n",
    "\n",
    "\n",
    "# Data\n",
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR inputs\n",
    "targets = np.array([[0], [1], [1], [0]])  # XOR targets\n",
    "\n",
    "\n",
    "# Forward pass with dummy data to build the model\n",
    "dummy_input = tf.random.normal((1, data.shape[1]))  # Match the shape of your input data\n",
    "_ = dfa_model(dummy_input, no_feedback=True)  # Perform a forward pass to build the model\n",
    "dfa_model.summary()\n",
    "\n",
    "# losses function and list\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "batch_size = 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "dataset = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "expected_results = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'epcoh: {epoch}')\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        x_batch = dataset[i:i + batch_size]\n",
    "        y_batch = expected_results[i:i + batch_size]\n",
    "        dfa_step(dfa_model, x_batch, y_batch, learning_rate)\n",
    "        # custom_backprop(dfa_model, x_batch, y_batch, learning_rate) # <= This works well\n",
    "\n",
    "    pred = dfa_model(x_batch, no_feedback=True)\n",
    "    batch_loss = loss_fn(y_batch, pred)\n",
    "    loss_history.append(batch_loss)\n",
    "\n",
    "pred = dfa_model(data, no_feedback=True)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    print(data[i], pred[i])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 1.035509 , -1.0815377]], dtype=float32)>, <tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
      "array([[ 0.4436265 ,  0.40576193,  1.6600581 , -1.455826  ,  1.3965882 ,\n",
      "        -0.0462802 , -0.69298774,  0.8014882 ,  0.27867788,  1.2127836 ,\n",
      "        -2.0153408 , -0.40689582,  0.04296972, -1.3317323 ,  1.4408419 ,\n",
      "         1.1257383 ,  0.16937268, -0.28693125,  0.12454437,  0.09126619,\n",
      "        -1.8368932 ,  0.2274011 ,  0.8482541 , -0.32945195, -2.290909  ,\n",
      "        -0.74049294, -1.6215048 , -0.06114914, -0.3074192 ,  0.70855474,\n",
      "        -0.04650941, -0.86513263, -0.71069723, -1.202043  ,  1.8484871 ,\n",
      "        -1.6053052 , -0.5683419 ,  0.5598701 ,  1.1364251 , -1.051593  ]],\n",
      "      dtype=float32)>]\n",
      "Matrix     Frobenius Norm       Rank       Condition Number     Min Value       Max Value       Average         Sum            \n",
      "0          1.497332             1          1.000000             -1.081538       1.035509        -0.023014       -0.046029      \n",
      "1          6.659543             1          1.000000             -2.290909       1.848487        -0.123768       -4.950727      \n"
     ]
    }
   ],
   "source": [
    "print(dfa_model.feedback_mat)\n",
    "\n",
    "# Assuming dfa_model has been instantiated and feedback_mat exists\n",
    "feedback_matrices = dfa_model.feedback_mat\n",
    "\n",
    "# Create a table header\n",
    "print(f\"{'Matrix':<10} {'Frobenius Norm':<20} {'Rank':<10} {'Condition Number':<20} {'Min Value':<15} {'Max Value':<15} {'Average':<15} {'Sum':<15}\")\n",
    "\n",
    "# Compute and print characteristics for each matrix\n",
    "for idx, mat in enumerate(feedback_matrices):\n",
    "    mat_np = mat.numpy()  # Convert to NumPy array if necessary\n",
    "    \n",
    "    # Frobenius Norm\n",
    "    frob_norm = np.linalg.norm(mat_np, 'fro')\n",
    "    \n",
    "    # Rank\n",
    "    rank = np.linalg.matrix_rank(mat_np)\n",
    "    \n",
    "    # Condition Number (use np.linalg.cond for non-square matrices)\n",
    "    try:\n",
    "        cond_num = np.linalg.cond(mat_np)\n",
    "    except np.linalg.LinAlgError:\n",
    "        cond_num = np.inf  # If the condition number cannot be computed, set to infinity\n",
    "    \n",
    "    # Min and Max Values\n",
    "    min_val = np.min(mat_np)\n",
    "    max_val = np.max(mat_np)\n",
    "    \n",
    "    # Average and Sum of all elements\n",
    "    avg_val = np.mean(mat_np)\n",
    "    total_sum = np.sum(mat_np)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{idx:<10} {frob_norm:<20.6f} {rank:<10} {cond_num:<20.6f} {min_val:<15.6f} {max_val:<15.6f} {avg_val:<15.6f} {total_sum:<15.6f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
